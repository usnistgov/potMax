\documentclass{article}

%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Details of potMax Analysis}

\usepackage{amssymb, amsmath}
\usepackage{url}

\begin{document}

\title{Details of \texttt{potMax} Analysis}

\author{Adam L. Pintar \and Dat Duthinh \and Emil Simiu}

\date{}

\maketitle

<<echo=FALSE>>=
library(potMax)
data("jp1tap1715wind270")
@


\section{Introduction}
\label{sec:intro}

The package \texttt{potMax} provides an estimation procedure for
the distribution of the peak (maximum) value of a stationary,
but otherwise fairly general, time series. It may also be used
to calculate return values. The model that forms the basis of the
procedure is a two dimensional Poisson process that is appropriate,
in an asymptotic sense, for modeling the extremes of a random process.
One dimension of the Poisson process is time since the target data
sets are time series. An example of the second dimension is the
pressure exerted by wind on a scale model of a building in a wind
tunnel. The motivation for this R package are such time series, and
the all sets provided with the package are examples of wind
tunnel data. This vignette has three goals:
\begin{enumerate}
\item Describe the Poisson process to be used, including justification
for its application to general random processes.
\item Provide the basic steps for estimation and uncertainty
quantification in general terms, i.e., not specific to R \cite{R} or
this package, \texttt{potMax}.
\item Show how the package \texttt{potMax} may be used to carry out
those steps.
\end{enumerate}

\section{Poisson Process}
\label{sec:poisson_process}

Poisson processes are defined by their intensity function.
The two intensity functions used here are given in Equations
\ref{eq:full_lambda} and \ref{eq:gumbel_lambda}.
\begin{align}\label{eq:full_lambda}
\lambda(t, y) = \frac{1}{\sigma}\left[
1 + \frac{k(y - \mu)}{\sigma}
\right]_+^{-1/k - 1}
\end{align}
\begin{align}\label{eq:gumbel_lambda}
\lambda(t, y) = \frac{1}{\sigma}\exp\left\{
\frac{-(y - \mu)}{\sigma}
\right\}
\end{align}
Notice that the left side of Equations \ref{eq:full_lambda} and
\ref{eq:gumbel_lambda} are functions of $t$ (time) and $y$ (e.g.,
pressure). However, the right side of Equations \ref{eq:full_lambda}
and \ref{eq:gumbel_lambda} are only functions of $y$. The
implication is that only stationary time series, those not changing
behavior over time, are considered. The $+$ subscript in Equation
\ref{eq:full_lambda} means that negative values inside the square
brackets are raised to zero. Equation \ref{eq:gumbel_lambda} is the
limit of Equation \ref{eq:full_lambda} as $k$ approaches zero. For
that reason, the Poisson process defined by the intensity function
in Equation \ref{eq:gumbel_lambda} is referred to as the Gumbel
model henceforth. The two dimensional Poisson processes defined
by Equations \ref{eq:full_lambda} and \ref{eq:gumbel_lambda} are
appropriate models for crossings of a high threshold. Consider
Figure \ref{fig:thresh_seq}, which depicts a raw time series and two
different thresholded versions. Notice in Figure \ref{fig:thresh_seq}
the "silos" of thresholded crossings. This occurs because the
observations comprising the raw time series are autocorrelated.
Figure \ref{fig:acf} depicts an estimate of the autocorrelation
function (ACF) for the series shown in the top plot of Figure
\ref{fig:thresh_seq}. Observations separated by more than 40
increments of time (in this case almost one tenth of a second) remain
highly positively correlated.

<<thresh_seq, echo = FALSE, fig.cap='Raw time series (top), observations above 1.8 (middle), and observations above 2 (bottom).'>>=
complete_series <- -jp1tap1715wind270$value
par(mfrow = c(3, 1), mar = c(0, 4.1, 4, 3))
plot(complete_series, type = 'l',
     xaxt = 'n',
     yaxt = 'n',
     ylab = '',
     xlab = '',
     ylim = c(0, 3.2))
axis(2, at = c(0, 1.5, 3))
par(mar = c(2, 4.1, 2, 3))
plot_y <- complete_series
plot_y[plot_y <= 1.8] <- NA
plot(plot_y, pch = 19,
     yaxt = 'n',
     xaxt = 'n',
     ylab = 'Pressure',
     xlab = '',
     ylim = c(0, 3.2))
abline(h = 1.8, lty = 'dashed')
axis(4, at = c(0, 1.5, 3))
par(mar = c(4, 4.1, 0, 3))
plot_y <- complete_series
plot_y[plot_y <= 2.0] <- NA
plot(plot_y, pch = 19,
     xaxt = 'n', yaxt = 'n',
     ylab = '', xlab = 'Seconds',
     ylim = c(0, 3.2))
abline(h = 2.0, lty = 'dashed')
axis(2, at = c(0, 1.5, 3))
axis(1,
     at = seq(from = 1,
              to = length(complete_series),
              length = 5),
     labels = seq(from = 0, to = 100, length = 5))
@

<<acf, echo = FALSE, fig.cap='Estimated autocorrelation function for the time series in the top plot of Figure \\ref{fig:thresh_seq}.'>>=
plot(acf(complete_series, plot = FALSE), main = '', bty = 'l')
@

Poisson process are not appropriate for such data without further
processing because one of their defining assumptions is independence.
I am deliberately being vague because a careful treatment of
Poisson processes is out of the scope of this vignette. So that
the Poisson process model is tenable, the time series must be
declustered before thresholding. Declustering proceeds by forming
clusters and discarding all but the cluster maximums. Clusters
are formed by groups of sequential observations falling above
the series mean value. All observations below the series mean
are ignored since the focus is on estimating the distribution
of the peak value. Figure \ref{fig:declust_thresh_seq} displays
the analog of Figure \ref{fig:thresh_seq} after declustering,
and Figure \ref{fig:acf_declustered} depicts the estimated
autocorrelation function of the series in the top plot of Figure
\ref{fig:declust_thresh_seq}. Figure \ref{fig:acf_declustered}
shows that the declustering is very effective for removing the
autocorrelation. After removing the autocorrelation in the series,
or declustering, the use of the Poisson processes defined by
the intensity functions in Equations \ref{eq:full_lambda} and
\ref{eq:gumbel_lambda} as models for crossings of a high threshold
is justified. They are used for such purposes in many papers, e.g.,
\cite{smith89, smith04, coles04, pintar_et_al15, duthinh_et_al17}. The
original theoretical justification is provided by \cite{pickands71}.

<<declust_thresh_seq, echo = FALSE, fig.cap='Raw time series (top), observations above 1.8 after declustering (middle), and observations above 2 after declustering (bottom).'>>=
par(mfrow = c(3, 1), mar = c(0, 4.1, 4, 3))
y <- decluster(complete_series, obs_times = (1:length(complete_series))/500)
plot(y$declustered_times, y$declustered_series, type = 'l',
     xaxt = 'n',
     yaxt = 'n',
     ylab = '',
     xlab = '',
     ylim = c(0, 3.2))
axis(2, at = c(0, 1.5, 3))
par(mar = c(2, 4.1, 2, 3))
plot_y <- y$declustered_series
plot_y[plot_y <= 1.8] <- NA
plot(y$declustered_times, plot_y, pch = 19,
     yaxt = 'n',
     xaxt = 'n',
     ylab = 'Pressure',
     xlab = '',
     ylim = c(0, 3.2))
abline(h = 1.8, lty = 'dashed')
axis(4, at = c(0, 1.5, 3))
par(mar = c(4, 4.1, 0, 3))
plot_y <- y$declustered_series
plot_y[plot_y <= 2.0] <- NA
plot(y$declustered_times, plot_y, pch = 19,
     yaxt = 'n',
     ylab = '', xlab = 'Seconds',
     ylim = c(0, 3.2))
abline(h = 2.0, lty = 'dashed')
axis(2, at = c(0, 1.5, 3))
@

<<acf_declustered, echo = FALSE, fig.cap='Estimated autocorrelation function for the time series in the top plot of Figure \\ref{fig:thresh_seq} after declustering it.'>>=
plot(acf(decluster(complete_series)$declustered_series, plot = FALSE), main = '', bty = 'l')
@

\subsection{Threshold Choice}
\label{sec:threshold_choice}

The intensity functions \ref{eq:full_lambda} and
\ref{eq:gumbel_lambda} are considered over the domain $\mathcal{D}
= (0, \infty)\times(b, \infty)$, where $b$ is a chosen threshold.
A hurdle to the use of these models is the appropriate choice of
$b$. Since the threshold dictates the data that are included in (or
omitted from) fitting the model, its impact on the results can be
large. Theory commands \cite{pickands71} that the model becomes more
appropriate as the threshold increases. However, since observations
are taken over a finite period of time, the threshold cannot be
chosen too high because too few observations will remain for fitting
the model. Any approach to choosing a threshold must balance these
competing aspects. A common and easy to implement approach is to
pick a high quantile of the series, e.g., 95\% (see page 489 of
\cite{m-s_et_al10}). This R package considers two alternatives.

\subsubsection{Optimal Threshold}
\label{sec:optimal_threshold}

An optimal threshold based on the fit of the model to the data
is the first alternative. The fit of the model is judged by the
$W$-statistics defined in Equation (1.30) of \cite{smith04}. Figure
\ref{fig:wplot} shows a plot of $W$-statistics versus quantiles of the
standard exponential distribution using the optimal threshold for the
series in the top plot of Figure \ref{fig:declust_thresh_seq}. If the
data fit perfectly to the model, the points would fall exactly on the
diagonal line. The threshold is chosen by creating such a plot for a
sequence of potential thresholds and selecting the one that minimizes
the maximum absolute vertical distance to the diagonal line. This
approach is used in \cite{duthinh_et_al17}.

<<wplot, echo = FALSE, fig.cap = 'Plot of the $W$-statistics versus their corresponding standard exponential quantiles for the declustered series depicted in the top plot of \\ref{fig:declust_thresh_seq} using the optimal threshold.'>>=
declustered_obs <- decluster(complete_series)

total_time <- 100 # seconds

thresholded_obs <- gumbelEstThreshold(x = declustered_obs,
                                      lt = total_time,
                                      n_min = 10,
                                      n_max = 50)

full_pot_fit <- gumbelMLE(x = thresholded_obs,
                          hessian_tf = TRUE)

gumbelWPlot(x = full_pot_fit,
            tf_plot = TRUE, BW = FALSE, details = FALSE,
            pch = 19)

@

\subsubsection{Many Thresholds}
\label{sec:many_thresholds}

The second alternative is to combine results for many thresholds by
weighted averaging. The key step then is constructing the weights, for
which the $W$-statistics may again be leveraged. Let $\delta_j$ be the
maximum absolute vertical distance in a plot of the $W$-statistics
against the standard exponential quantiles, normalized to the unit
interval for the threshold $b_j$. A natural transformation of the
$\delta_j$ is
\begin{align}
 w_j = \frac{\exp\{-\tau\delta_j\}}{\sum_j\exp\{-\tau\delta_j\}}
\end{align}
With $\tau = 0$, each threshold recieves equal weight, and
as $\tau$ approaches $\infty$, the weight corresponding to the
smallest $\delta_j$ approaches unity while the rest approach
zero. A reasonable setting seems to be $\tau = 5$
\cite{pintar_duthinh_prep}.

\subsection{Estimation}
\label{sec:estimation}

The model parameters, $\eta$=($\mu$, $\sigma$, $k$) for the
intensity function in Equation \ref{eq:full_lambda}, and
$\eta$=($\mu$, $\sigma$) for the intensity function in Equation
\ref{eq:gumbel_lambda} are estimated by maximum likelihood. Let $(y_i,
t_i)$ be the declustered values that lie above the threshold $b$. The
likelihood is then given by Equation \ref{eq:likelihood}.
\begin{align}\label{eq:likelihood}
L(\eta) = \left(
\prod_{i = 1}^I \lambda(t_i, y_i)
\right)\cdot
\exp\left\{
-\int_{\mathcal{D}} \lambda(t, y)dtdy
\right\}
\end{align}
The domain of integration, $\mathcal{D}$, in Equation
\ref{eq:likelihood} is the unbounded rectangle $[0, T]\times [b,
\infty)$, where $T$ is the time of the last observation.

\section{Estimating the Distribution of the Peak}
\label{sec:est_peak_dist}

The goal of this section is to describe the algorithm for estimating
the distribution of the peak in general statistical terms, i.e.,
without reference to R or the \texttt{potMax} package. It is expressed
as four steps, most with substeps of their own.
\begin{enumerate}
\item Decluster the series.
\item Select the threshold or thresholds
  \begin{enumerate}
  \item Construct a set of potential thresholds. This may be done by
  specifying a minimum and maximum number of observations and identifying
  their respective thresholds, say $b_1$ and $b_B$. Then, the thresholds
  are $b_1 > b_2 > \cdots > b_{B - 1} > b_B$ such that if $b_j$
  corresponds to $n_j$ observations, $b_{j + 1}$ corresponds to $n_j +
  1$ observations (assuming no ties).
  \item For each potential threshold, fit the model via maximum
  likelihood as described in Section \ref{sec:estimation}.
  \item Create a $W$-plot for each fit, as described in Section
  \ref{sec:optimal_threshold}.
  \item Summarize each $W$-plot by the maximum absolute vertical
  distance from the points to the 45$^{\circ}$ line
  \item Select the threshold that minimizes the maximum distance as
  described in Section \ref{sec:optimal_threshold}, or use the distances
  to specify a weight associated with each threshold as described in
  Section \ref{sec:many_thresholds}
  \end{enumerate}
\item Empirically build the distribution of the peak
  \begin{enumerate}
  \item Generate a series of desired length from the fitted model
  \begin{itemize}
    \item This may be accomplished, for example, by algorithm 9 of
    \cite{pasupathy11}
    \item If results of many thresholds are being combined, select a
    threshold at random from the collection according to the weights,
    i.e., threholds with high weights should be selected more often.
  \end{itemize}
  \item Record the peak of the generated series
  \item Repeat (a) and (b) $n_{mc}$ times; the recorded peaks form an
  empirical approximation to the distribution of the peak, or mixture of
  peak distributions in the case of many thresholds.
  \end{enumerate}
\item Quantify uncertainty
  \begin{itemize}
  \item For a single optimal threhold
    \begin{enumerate}
    \item Sample $n_{boot}$ values from a multivariate Gaussian
    distribution with mean equal to the estimated parameters,
    $\widehat{\eta}$, and covariance matrix equal to the negative inverse
    Hessian matrix of the log-likilihood evaluated at its maximum
    \item For each set of sampled parameters reapeat step 3
    \end{enumerate}
  \item For a collection of thresholds
    \begin{enumerate}
    \item Sample the declustered data with replacement (a bootstrap
    sample)
    \item For each potential threshold refit the model
    \item Recalculate the weights
    \item Repeat step 3
    \item Repeat (a) -- (d) $n\_boot$ times
    \end{enumerate}
  \end{itemize}
\end{enumerate}
The result of step 4 is $n_{boot}$ empirical approximations to the
distribution of the peak, or mixture of peak distributions in the case
of many thresholds.

\section{Return Values}
\label{sec:return_values}

Calculation of return values follows an algorithm similar to that
in Section \ref{sec:est_peak_dist}. The only difference is step 3.
Instead of empirically building the peak distribution, the equation
\begin{align}
\label{eq:return_val_eq}
  \int_{y_N}^\infty\int_0^1 \lambda(t, y)dtdy = \frac{1}{N}
\end{align}
is solved for $y_N$, which is interpreted as the $N$-year, -month,
-week, etc. return value. In the case of many thresholds, Equation
\ref{eq:return_val_eq} is sloved for each threshold and the solutions
are combined by weighted averaging according the weights calculated in
step 2.

\section{Using \texttt{potMax}}
\label{sec:using_potMax}

The \texttt{potMax} package is demonstrated on the data in the top
plot of Figure \ref{fig:thresh_seq}. The data set is distributed
as part of the \texttt{potMax} package and is referenced by the
name \texttt{jp1tap1715wind270}. The present sections describe
the functions to call to carryout the steps described in Section
\ref{sec:est_peak_dist}.

\subsection{Declustering}
\label{sec:declustering_step}

<<echo = TRUE>>=
complete_series <- -jp1tap1715wind270$value
declustered_obs <- decluster(complete_series = complete_series)
@

The argument \texttt{complete\_series} is the time series itself.
Note the negative sign because deep valleys are more interesting for
this data set than high peaks. There exists a second optional argument
to the function \texttt{decluster}. See the help file for more
information.

\subsection{Threshold Selection and Calculation of Weights}
\label{sec:thresh_choice_weights}

<<echo = TRUE, include = TRUE>>=
thresholded_obs <- gumbelEstThreshold(x = declustered_obs,
                                      lt = 100,
                                      n_min = 10,
                                      n_max = 100)
summary(thresholded_obs)

pot_fit <- gumbelMLE(x = thresholded_obs, hessian_tf = TRUE)
@

The first argument \texttt{x} is the output from the function
\texttt{decluster}. The second argument, \texttt{lt}, is the length
of observation time for the series, in this case 100 seconds. The
unit for time can be anything. It is the burden of the user to be
consistent and interpret results according to the supplied unit.
The third argument, \texttt{n\_min}, specifies the minimum number
of observations to be allowed after thresholding. This argument
affects how many potential thresholds are considered in the search
for an optimal threshold. For the Gumbel model, I recommend 10 or
more, and for the full model, I recommend 15 or more. The fourth
argument, \texttt{n\_max}, complements \texttt{n\_min}, specifying
the maximum number of observations to allow after thresholding.
This should be chosen sufficiently high that the selected threshold
does not correspond to that number of observations, but not so
high that run times become unbearable. Note that the output of
\texttt{gumbelEstThreshold} must be passed into \texttt{gumbelMLE}
once more so that the Hessian matrix may be calculated, which is not
done by \texttt{gumbelEstThreshold}. This is necessary for uncertainty
quantification.

The $W$-plot for the fitted model can be creted with the function
\texttt{gumbelWPlot}. See the help file for \texttt{gumbelWplot} for a
description of its arguments. The plot for the example series is found
in Figure \ref{fig:wplot}.

<<echo = TRUE, include = FALSE, eval = FALSE>>=
gumbelWPlot(x = pot_fit,
            tf_plot = TRUE, BW = FALSE, details = FALSE)
@

A plot of all proposed thresholds against the corresponding fit statistic
is available too, Figure \ref{fig:thresh_plot}.

<<thresh_plot, echo = TRUE, fig.cap = 'Plot of fit statistics versus thresholds.  The red point identifies the minimum.'>>=
threshPlot(thresholded_obs)
@

If it is desired to combine the results from multiple thresholds,
the function \texttt{gumbelMultiFit} is called in place of both,
\texttt{gumbelEstThreshold} and \texttt{gumbelMLE}. The required
arguments are the same as for \texttt{gumbelEstThreshold}, except the
additional \texttt{weight\_scale}, for which a good choice is 5, as
described in Section \ref{sec:many_thresholds}

<<echo=TRUE, include=TRUE>>=
multi_pot_fit <- gumbelMultiFit(x = declustered_obs, lt = 100,
                                n_min = 10, n_max = 100,
                                weight_scale = 5)
summary(multi_pot_fit)
@

\subsection{Estimation of the Distribution of the Peak}
\label{sec:est_peak_dist}

The call to \texttt{gumbelMaxDist} is the same for a single optimal
threshold and many thresholds.

<<echo = TRUE>>=
max_dist <- gumbelMaxDist(x = pot_fit,
                          lt_gen = 200,
                          n_mc = 1000,
                          progress_tf = FALSE)

multi_max_dist <- gumbelMaxDist(x = multi_pot_fit,
                                lt_gen = 200,
                                n_mc = 1000,
                                progress_tf = FALSE)
@

The argument \texttt{lt\_gen} provides the length of the series
for which the distibutio of the maximum is sought, which could be
different from the lenght of the original series, but with consistent
units. The argument \texttt{n\_mc} is the number of samples to draw
from the distribution of the maximum. A progress bar may optionally be
drawn.

For a single optimal threshold as well as many thresholds, the mean
of the distribution may be calculated with the function \texttt{mean},
and the entire distribution may be plotted as a histogram with an S3
method for the generic \texttt{plot} function.

<<max_dist_hist, echo = TRUE, fig.cap = 'Histogram of the estimated distribution of the peak value considering only a single optimal threshold.  The red point shows the mean of the distribution.'>>=
mean(max_dist)
plot(max_dist)
@

<<multi_max_dist_hist, echo = TRUE, fig.cap = 'Histogram of the estimated distribution of the peak value combining results from many thresholds.  The red point shows the mean of the distribution.'>>=
mean(multi_max_dist)
plot(multi_max_dist)
@

\subsection{Uncertainty}

<<echo = TRUE>>=
max_dist_uncert <- gumbelMaxDistUncert(x = pot_fit,
                                       lt_gen = 200,
                                       n_mc = 1000,
                                       n_boot = 200,
                                       progress_tf = FALSE)

multi_max_dist_uncert <- gumbelMaxDistUncert(x = multi_pot_fit,
               declust_obs = declustered_obs$declustered_series,
                                             lt_gen = 200,
                                             n_mc = 1000,
                                             n_boot = 200,
                                             progress_tf = FALSE)
@

The argument \texttt{n\_boot} corresponds to the number of bootstrap
replicates of the peak distribution to calculate. Note that the
\texttt{lt\_gen} argument in the call to \texttt{gumbelMaxDistUncert}
should match the \texttt{lt\_gen} argument provided in the
call to \texttt{gumbelMaxDist}. All other arguments are as previously
described. For the \texttt{gumbel\_multi\_fit} S3 method, the
original declustered observations must be passed in as an argument so
that boostrap samples may be constructed.

The results maybe added to the histogram depicting the
distribution of the peak (Figure 10),% \ref{fig:uncert_added_to_hist}),
or they may be displayed alone (Figure \ref{fig:uncert_alone}).
The same \texttt{plot} commands work for the S3
classes \texttt{gumbel\_max\_dist\_multi\_thresh} and
\texttt{gumbel\_max\_dist\_uncert\_multi\_thresh}.

<<echo = FALSE, include = FALSE>>=
plot(max_dist)
@


<<uncert_added_to_hist, echo = TRUE, fig.cap = 'Same as Figure \\ref{fig:max_dist_hist}, but adding bootstrap replictes of the distribution of the peak value to convey uncertainty in estimation.  The red line shows an 80\\% confidence interval for the mean of the distribution of the peak.', out.height = '4in'>>=
# Figure 9
plot(max_dist_uncert, add = TRUE)
@

<<uncert_alone, echo = TRUE, fig.cap = 'Bootstrap replicates of the distribution of the peak starting with the series shown in the top plot of Figure \\ref{fig:thresh_seq}.  The red line shows an 80\\% confidence interval for the mean of the distribution of the peak value.'>>=
# Figure 10
plot(max_dist_uncert, add = FALSE)
@

\subsection{Return Values}
\label{sec:calc_return_values}

The calls to \texttt{gumbelMaxDist} and \texttt{gumbelMaxDistUncert}
are replaced by \texttt{gumbelNYear} and \texttt{gumbelNYearUncert}.
Plot and summary functions are not always implemented for return
values.

<<echo=TRUE, include=TRUE>>=
return_200s <- gumbelNYear(x = pot_fit, N = 200)
return_200s$N_year_val
return_200s_uncert <- gumbelNYearUncert(x = pot_fit, N = 200,
                                        n_boot = 200)
quantile(return_200s_uncert$boot_samps, probs = c(0.1, 0.9))
multi_return_200s <- gumbelNYear(x = multi_pot_fit, N = 200)
multi_return_200s$N_year_val
multi_return_200s_uncert <- gumbelNYearUncert(x = multi_pot_fit,
               declust_obs = declustered_obs$declustered_series,
                                              N = 200,
                                              n_boot = 200)
summary(multi_return_200s_uncert)
@


\section{Full Estimation}
\label{sec:full_estimation}

The steps shown in Section \ref{sec:using_potMax} leveraged the
intensity function in Equation \ref{eq:gumbel_lambda}. To use the
intensity function in Equation 1 instead, replace the function
calls \texttt{gumbel*} with \texttt{full*}. For example, to choose
an optimal threshold, use the function \texttt{fullEstThreshold}.
Note that the functions \texttt{fullEstThreshold}, \texttt{fullMLE},
and \texttt{fullMultiFit} have an additional required argument,
\texttt{n\_starts}, which specifies the number of times to perform
the optimization from random starting locations. The method for
\texttt{fullMaxDistUncert} for the S3 class \texttt{full\_multi\_fit}
also requires this argument. A reasonable value is 20. \textit{Note
that functions associated with return values are only implemented
for the the Gumbel model, i.e. the intensity function in Equation
\ref{eq:gumbel_lambda}}.

\begin{thebibliography}{Pintar and Duthinh, in prep}

\bibitem[Coles, 2004]{coles04} Coles, S., ``The use and Misuse of Extreme
Value Models in Practice,'' in \textit{Extreme Values in Finance,
Telecomunications, and the Environment}, Finkenst\"{a}dt, B. and
Rootz\'{e}n, H., editors, chapter 2, 79--100, Chapman \& Hall/CRC
(2004).

\bibitem[Duthinh et al., 2017]{duthinh_et_al17} Duthinh, D., Pintar,
A.L., and Simiu, E. ``Estimating peaks of stationary random processes:
A peaks-over-threshold approach,'' \textit{ASCE-ASME Journal of Risk
and Uncertainty in Engineering Systems, Part A: Civil Engineering}
\textbf{3}(4), URL \url{https://doi.org/10.1061/AJRUA6.0000933}
(2017).

\bibitem[Mannshardt et al., 2010]{m-s_et_al10} Mannshardt-Shamseldin, E. C.,
Smith, R. L., Sain, S. R., Mearns, L. O., and Cooley, D., ``Downscaling
Extremes: A Comparison of Extremem Value Distributions in Point-source
and gridded Precipitation Data,'' \textit{The Annals of Applied
Statistics}, \textbf{4}, 484--502 (2010).

\bibitem[Nelder and Mead, 1965]{nelder_mead65} Nelder, J. A. and Mead, R., ``A
Simplex Method for Function Minimization,'' \textit{The Computer
Journal}, \textbf{7}, 308--313 (1965).

\bibitem[Pasupathy, 2011]{pasupathy11} Pasupathy, R., ``Generating
nonhomogeneous Poisson processes.'' In \textit{Wiley encyclopedia
of operations research and management science} , Hoboken, NJ: Wiley
(2011).

\bibitem[Pickands, 1971]{pickands71} Pickands, J. III, ``The Two-dimensional
Poisson Process and Extremal Processes,'' \textit{Journal of Applied
Probability}, \textbf{8}, 745--756 (1971).

\bibitem[Pintar et al., 2015]{pintar_et_al15} Pintar, A. L., Simiu, E., Lombardo, F.
T., and Levitan, M., ''Maps of Non-hurricane Non-tornadic Winds Speeds
With Specified Mean Recurrence Intervals for the Contiguous United
States Using a Two-dimensional Poisson Process Extreme Value Model
and Local Regression,'' \textit{NIST Special PUblication 500-301}, URL
\url{http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.500-301.pdf}
(2015).

\bibitem[Pintar and Duthinh, in prep]{pintar_duthinh_prep} Pintar, A.
L. and Duthinh, D., ``Threshold Uncertainty in Poisson Process Extreme
Vaue Models,'' (in preparation).

\bibitem[R, 2018]{R} R Core Team, ``R: A Language and Environment for
Statistical Computing,'' R Foundation for Statistical Computing,
Vienna, Austria, URL \url{https://www.R-project.org/} (2018).

\bibitem[Smith, 1989]{smith89} Smith, R. L., ``Extreme Value Analysis of
Environmental Time Series: an Application to Trend Detection in
Ground-level Ozone,'' \textit{Statistical Science}, \textbf{4},
367--393 (1989).

\bibitem[Smith, 2004]{smith04} Smith, R. L., ``Statistics of Extremes,
with Applications in Environment, Insurance, and Finance,'' in
\textit{Extreme Values in Finance, Telecomunications, and the
Environment}, Finkenst\"{a}dt, B. and Rootz\'{e}n, H., editors,
chapter 1, 1--78, Chapman \& Hall/CRC (2004).

\end{thebibliography}

\end{document}